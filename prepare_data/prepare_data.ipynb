{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from sklearn import preprocessing    \n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writetrainann(file):\n",
    "    maxl = 0\n",
    "    columns = ['name', 'label']\n",
    "    names = []\n",
    "    labels = []\n",
    "    categories = []\n",
    "    with open(file, 'r') as f:\n",
    "        ann = json.loads(f.read())\n",
    "        for key in ann.keys():\n",
    "            img_dict = ann[key]\n",
    "            img_name = img_dict['name']\n",
    "            cat_list = img_dict['category']\n",
    "            maxl = max(maxl, len(cat_list))\n",
    "            categories.extend(cat_list)\n",
    "            \n",
    "            for cat in cat_list:\n",
    "                names.append(img_name)\n",
    "                labels.append(cat)\n",
    "                \n",
    "        data = pd.DataFrame(list(zip(names, labels)), columns=columns)\n",
    "        print('Maximum categories per image', maxl)\n",
    "        print('Total number of categories', len(set(categories)))\n",
    "        print('All categories', set(categories))\n",
    "        data.to_csv('../Data/train_ann.csv', index = False)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum categories per image 9\n",
      "Total number of categories 79\n",
      "All categories {'frisbee', 'microwave', 'chair', 'orange', 'cup', 'keyboard', 'scissors', 'vase', 'hot dog', 'cell phone', 'stop sign', 'car', 'tv', 'laptop', 'skis', 'bottle', 'sink', 'broccoli', 'fork', 'book', 'horse', 'sports ball', 'tie', 'skateboard', 'truck', 'toothbrush', 'sandwich', 'cake', 'bowl', 'bicycle', 'dog', 'spoon', 'bird', 'apple', 'tennis racket', 'bear', 'suitcase', 'dining table', 'parking meter', 'couch', 'elephant', 'backpack', 'banana', 'remote', 'umbrella', 'sheep', 'clock', 'giraffe', 'wine glass', 'knife', 'bus', 'baseball bat', 'handbag', 'bed', 'cat', 'baseball glove', 'traffic light', 'kite', 'carrot', 'fire hydrant', 'hair drier', 'train', 'potted plant', 'teddy bear', 'donut', 'toaster', 'pizza', 'bench', 'surfboard', 'toilet', 'mouse', 'motorcycle', 'airplane', 'cow', 'zebra', 'refrigerator', 'snowboard', 'boat', 'oven'}\n"
     ]
    }
   ],
   "source": [
    "ann_dir = \"../../../Data/annotation\"\n",
    "train_fname = 'i2l_trainset_annotation.json'\n",
    "train_fname = os.path.join(ann_dir, train_fname)\n",
    "train_data = writetrainann(train_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Freq category: chair\n",
      "Min Freq category: bear\n",
      "Mean Freq: 599.620253164557\n",
      "                name\n",
      "label               \n",
      "airplane         141\n",
      "apple            290\n",
      "backpack        1394\n",
      "banana           293\n",
      "baseball bat     590\n",
      "baseball glove   710\n",
      "bear              44\n",
      "bed              197\n",
      "bench            925\n",
      "bicycle          615\n",
      "bird             397\n",
      "boat             275\n",
      "book            1164\n",
      "bottle          2190\n",
      "bowl            1220\n",
      "broccoli         207\n",
      "bus              400\n",
      "cake             263\n",
      "car             2139\n",
      "carrot           247\n",
      "cat              277\n",
      "cell phone      1277\n",
      "chair           2243\n",
      "clock           1145\n",
      "couch            568\n",
      "cow               99\n",
      "cup             2241\n",
      "dining table     762\n",
      "dog              583\n",
      "donut            135\n",
      "...              ...\n",
      "pizza            127\n",
      "potted plant     957\n",
      "refrigerator     570\n",
      "remote           646\n",
      "sandwich         200\n",
      "scissors         191\n",
      "sheep             56\n",
      "sink            1073\n",
      "skateboard       463\n",
      "skis             364\n",
      "snowboard        248\n",
      "spoon            919\n",
      "sports ball     1301\n",
      "stop sign        394\n",
      "suitcase         318\n",
      "surfboard        508\n",
      "teddy bear       243\n",
      "tennis racket    682\n",
      "tie              685\n",
      "toaster           75\n",
      "toilet           570\n",
      "toothbrush       227\n",
      "traffic light    799\n",
      "train            126\n",
      "truck           1113\n",
      "tv              1006\n",
      "umbrella         468\n",
      "vase             661\n",
      "wine glass       502\n",
      "zebra             67\n",
      "\n",
      "[79 rows x 1 columns]\n",
      "{'dog': 28, 'spoon': 60, 'refrigerator': 51, 'bottle': 13, 'book': 12, 'chair': 22, 'bowl': 14, 'car': 18, 'handbag': 36, 'traffic light': 71, 'oven': 47, 'potted plant': 50, 'bench': 8, 'sink': 56, 'vase': 76, 'dining table': 27, 'cup': 26, 'microwave': 43, 'clock': 23, 'tv': 74, 'toothbrush': 70, 'fork': 32, 'orange': 46, 'keyboard': 39, 'teddy bear': 65, 'mouse': 45, 'backpack': 2, 'wine glass': 77, 'knife': 41, 'toaster': 68, 'remote': 52, 'laptop': 42, 'cell phone': 21, 'couch': 24, 'toilet': 69, 'bird': 10, 'cat': 20, 'horse': 37, 'banana': 3, 'sports ball': 61, 'truck': 73, 'apple': 1, 'scissors': 54, 'bicycle': 9, 'bed': 7, 'hair drier': 35, 'motorcycle': 44, 'sandwich': 53, 'tie': 67, 'umbrella': 75, 'frisbee': 33, 'stop sign': 62, 'boat': 11, 'bus': 16, 'parking meter': 48, 'train': 72, 'fire hydrant': 31, 'airplane': 0, 'broccoli': 15, 'cake': 17, 'suitcase': 63, 'giraffe': 34, 'skis': 58, 'snowboard': 59, 'kite': 40, 'skateboard': 57, 'surfboard': 64, 'tennis racket': 66, 'carrot': 19, 'cow': 25, 'sheep': 55, 'hot dog': 38, 'elephant': 30, 'zebra': 78, 'donut': 29, 'baseball glove': 5, 'bear': 6, 'baseball bat': 4, 'pizza': 49}\n"
     ]
    }
   ],
   "source": [
    "# Checking training examples per-class. Dataset Stats\n",
    "\n",
    "train_data = pd.read_csv('../Data/train_ann.csv')\n",
    "train_groups = train_data.groupby(['label'])\n",
    "count_df = train_groups.agg('count')\n",
    "\n",
    "print('Max Freq category:', train_data['label'].value_counts().idxmax())\n",
    "print('Min Freq category:',train_data['label'].value_counts().idxmin())\n",
    "print('Mean Freq:',train_data['label'].value_counts().mean())\n",
    "print(count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding labels (0-78)\n",
    "\n",
    "train_data = pd.read_csv('../Data/train_ann.csv')\n",
    "le = preprocessing.LabelEncoder()\n",
    "train_data['label_enc'] = le.fit_transform(train_data['label'])\n",
    "train_data.to_csv('../Data/train_ann_encoded.csv', index = False)   \n",
    "\n",
    "label_enc_dict = pd.Series(train_data.label_enc.values,index=train_data.label).to_dict()\n",
    "print(label_enc_dict)\n",
    "\n",
    "label_enc_json = json.dumps(label_enc_dict)\n",
    "f = open(\"../Data/labels.json\",\"w\")\n",
    "f.write(label_enc_json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeann_multilabel(file):\n",
    "    maxl = 0\n",
    "    columns = ['name', 'label']\n",
    "    names = []\n",
    "    labels = []\n",
    "    with open(file, 'r') as f:\n",
    "        ann = json.loads(f.read())\n",
    "        for key in ann.keys():\n",
    "            img_dict = ann[key]\n",
    "            img_name = img_dict['name']\n",
    "            cat_list = img_dict['category']\n",
    "            maxl = max(maxl, len(cat_list))\n",
    "            category = \" \".join(cat_list)\n",
    "            names.append(img_name)\n",
    "            labels.append(category)\n",
    "        \n",
    "        data = pd.DataFrame(list(zip(names, labels)), columns=columns)\n",
    "        data.to_csv('../Data/train_ml_ann.csv', index = False)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    name                   label\n",
      "0  000000016164_left.png                     dog\n",
      "1  000000471175_left.png                     dog\n",
      "2  000000026654_left.png                     dog\n",
      "3  000000158497_left.png  spoon refrigerator dog\n",
      "4  000000390348_left.png                     dog\n",
      "5  000000369190_left.png                     dog\n",
      "6  000000151988_left.png              bottle dog\n",
      "7  000000307993_left.png                book dog\n",
      "8  000000007125_left.png                     dog\n",
      "9  000000346965_left.png                     dog\n"
     ]
    }
   ],
   "source": [
    "ann_dir = \"../../../Data/annotation\"\n",
    "train_fname = 'i2l_trainset_annotation.json'\n",
    "train_fname = os.path.join(ann_dir, train_fname)\n",
    "writeann_multilabel(train_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeann_multilabel_enc(file):\n",
    "    \n",
    "    with open(\"../Data/labels.json\",\"r\") as f:\n",
    "        encodings = json.loads(f.read())\n",
    "    \n",
    "    maxl = 0\n",
    "    columns = ['name', 'label']\n",
    "    names = []\n",
    "    labels = []\n",
    "    with open(file, 'r') as f:\n",
    "        ann = json.loads(f.read())\n",
    "        for key in ann.keys():\n",
    "            img_dict = ann[key]\n",
    "            img_name = img_dict['name']\n",
    "            cat_list = img_dict['category']\n",
    "            maxl = max(maxl, len(cat_list))\n",
    "            cat_labels = []\n",
    "            for cat in cat_list:\n",
    "                cat_labels.append(encodings[cat])\n",
    "            \n",
    "            category = cat_labels\n",
    "            names.append(img_name)\n",
    "            labels.append(category)\n",
    "        \n",
    "        data = pd.DataFrame(list(zip(names, labels)), columns=columns)\n",
    "        print(data.head(5))\n",
    "        data.to_csv('../Data/train_ml_ann_encoded.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    name         label\n",
      "0  000000016164_left.png          [28]\n",
      "1  000000471175_left.png          [28]\n",
      "2  000000026654_left.png          [28]\n",
      "3  000000158497_left.png  [60, 51, 28]\n",
      "4  000000390348_left.png          [28]\n"
     ]
    }
   ],
   "source": [
    "# Applying label encoding on trainig_multilabel data\n",
    "\n",
    "ann_dir = \"../../../Data/annotation\"\n",
    "train_fname = 'i2l_trainset_annotation.json'\n",
    "train_fname = os.path.join(ann_dir, train_fname)\n",
    "writeann_multilabel_enc(train_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writevalann(file):\n",
    "    maxl = 0\n",
    "    columns = ['name', 'label', 'WC1', 'WC2', 'WC3', 'WC4']\n",
    "    names = []\n",
    "    labels = []\n",
    "    WC1 = []\n",
    "    WC2 = []\n",
    "    WC3 = []\n",
    "    WC4 = []\n",
    "    with open(file, 'r') as f:\n",
    "        ann = json.loads(f.read())\n",
    "        for key in ann.keys():\n",
    "            img_dict = ann[key]\n",
    "            img_name = img_dict['name']\n",
    "            correct_cand = img_dict['correct_candidate']\n",
    "            wrong_cands = img_dict['wrong_candidate'] # four element always\n",
    "            names.append(img_name)\n",
    "            labels.append(correct_cand[0]) # only one element always\n",
    "            WC1.append(wrong_cands[0])\n",
    "            WC2.append(wrong_cands[1])\n",
    "            WC3.append(wrong_cands[2])\n",
    "            WC4.append(wrong_cands[3])\n",
    "    \n",
    "        data = pd.DataFrame(list(zip(names, labels, WC1, WC2, WC3, WC4)), columns=columns)\n",
    "        data.to_csv('../Data/val_ann.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    name          label       WC1         WC2            WC3  \\\n",
      "0  000000116358_left.png    sports ball     sheep       apple       backpack   \n",
      "1  000000050179_left.png  tennis racket  airplane    elephant            tie   \n",
      "2  000000523571_left.png   dining table       car     frisbee     cell phone   \n",
      "3  000000531798_left.png           bowl      cake  wine glass     teddy bear   \n",
      "4  000000531798_left.png         banana    toilet         cup  parking meter   \n",
      "\n",
      "     WC4  \n",
      "0  mouse  \n",
      "1   kite  \n",
      "2  pizza  \n",
      "3  clock  \n",
      "4   vase  \n"
     ]
    }
   ],
   "source": [
    "ann_dir = \"../../../Data/annotation\"\n",
    "val_fname = 'i2l_valset_annotation.json'\n",
    "val_fname = os.path.join(ann_dir, val_fname)\n",
    "writevalann(val_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writetestann(file):\n",
    "    maxl = 0\n",
    "    columns = ['name', 'label', 'WC1', 'WC2', 'WC3', 'WC4']\n",
    "    names = []\n",
    "    labels = []\n",
    "    WC1 = []\n",
    "    WC2 = []\n",
    "    WC3 = []\n",
    "    WC4 = []\n",
    "    with open(file, 'r') as f:\n",
    "        ann = json.loads(f.read())\n",
    "        for key in ann.keys():\n",
    "            img_dict = ann[key]\n",
    "            img_name = img_dict['name']\n",
    "            correct_cand = img_dict['correct_candidate']\n",
    "            wrong_cands = img_dict['wrong_candidate'] # four element always\n",
    "            names.append(img_name)\n",
    "            labels.append(correct_cand[0]) # only one element always\n",
    "            WC1.append(wrong_cands[0])\n",
    "            WC2.append(wrong_cands[1])\n",
    "            WC3.append(wrong_cands[2])\n",
    "            WC4.append(wrong_cands[3])\n",
    "    \n",
    "        data = pd.DataFrame(list(zip(names, labels, WC1, WC2, WC3, WC4)), columns=columns)\n",
    "        data.to_csv('../Data/test_ann.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_dir = \"../../../Data/annotation\"\n",
    "test_fname = 'i2l_testset_annotation.json'\n",
    "test_fname = os.path.join(ann_dir, test_fname)\n",
    "writetestann(test_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying label encoding on test and validation data (using labels applied for training)\n",
    "val_data = pd.read_csv('../Data/val_ann.csv')\n",
    "test_data = pd.read_csv('../Data/test_ann.csv')\n",
    "\n",
    "with open(\"../Data/labels.json\",\"r\") as f:\n",
    "    encodings = json.loads(f.read())\n",
    "    for col in val_data.columns[1:]:\n",
    "        val_data[col].replace(encodings, inplace=True)\n",
    "        \n",
    "    for col in test_data.columns[1:]:\n",
    "        test_data[col].replace(encodings, inplace=True)\n",
    "\n",
    "val_data.to_csv('../Data/val_ann_encoded.csv', index = False)  \n",
    "test_data.to_csv('../Data/test_ann_encoded.csv', index = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_hybrid_left_labels(file):\n",
    "    \n",
    "    columns = ['name', 'label']\n",
    "    names = []\n",
    "    labels = []\n",
    "    with open(file, 'r') as f:\n",
    "        ann = json.loads(f.read())\n",
    "        for key in ann.keys():\n",
    "            img_dict = ann[key]\n",
    "            img_name = img_dict['name']\n",
    "            cat_list = img_dict['category_symm']\n",
    "    \n",
    "            cat_labels = []\n",
    "            for cat in cat_list:\n",
    "                cat_labels.append(cat)\n",
    "            \n",
    "            category = cat_labels\n",
    "            names.append(img_name)\n",
    "            labels.append(category)\n",
    "        \n",
    "        data = pd.DataFrame(list(zip(names, labels)), columns=columns)\n",
    "        print(data.head(5))\n",
    "        data.to_csv('../Data/hybrid_train_ann_left_labels.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f0accc8ab761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mann_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../../../Data/annotation\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhybrid_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'hybrid_trainset_annotation.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhybrid_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# write_hybrid_left_labels(hybrid_fname)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Store left side image labels as list\n",
    "\n",
    "ann_dir = \"../../../Data/annotation\"\n",
    "hybrid_fname = 'hybrid_trainset_annotation.json'\n",
    "hybrid_fname = os.path.join(ann_dir, train_fname)\n",
    "# write_hybrid_left_labels(hybrid_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCorrMatrix(labels, encodings):\n",
    "    C = len(encodings.keys())\n",
    "\n",
    "    correlation_mat = np.zeros((C,C))\n",
    "    nrows = labels.shape[0]\n",
    "    print('classes: ',C)\n",
    "    print('images: ',nrows)\n",
    "    \n",
    "    nums = np.zeros(C)\n",
    "\n",
    "    for r in range(nrows):\n",
    "        list_label = literal_eval(labels.iloc[r])\n",
    "        n = len(list_label)\n",
    "        for a in range(n):\n",
    "            i = encodings[list_label[a]]\n",
    "            nums[i] += 1\n",
    "            for b in range(a + 1, n):\n",
    "                j = encodings[list_label[b]]\n",
    "                correlation_mat[i][j] += 1\n",
    "                correlation_mat[j][i] += 1\n",
    "                \n",
    "    return correlation_mat, nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:  79\n",
      "images:  32000\n"
     ]
    }
   ],
   "source": [
    "# Generating correlation matrix for ML-GCN\n",
    "\n",
    "hybrid_csv = pd.read_csv('../Data/hybrid_train_ann_left_labels.csv')\n",
    "labels = hybrid_csv['label']\n",
    "with open(\"../Data/labels.json\",\"r\") as f:\n",
    "        encodings = json.loads(f.read())\n",
    "\n",
    "adj, nums = getCorrMatrix(labels, encodings)\n",
    "\n",
    "corrMat = {}\n",
    "corrMat['adj'] = adj\n",
    "corrMat['nums'] = nums\n",
    "\n",
    "with open('../Data/baseline_left_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(corrMat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  87.  280.  877.  278.  398.  487.    9.  144.  690.  544.  238.  246.\n",
      " 1296. 2117. 1510.  224.  378.  329. 2246.  310.  165.  735. 3305.  487.\n",
      "  730.  119. 2302.  882.  280.  175.   60.  129.  737.  131.   49.   21.\n",
      " 1286.  172.  118.  340.  211.  860.  563.  325.  377.  221.  262.  505.\n",
      "   78.  256.  948.  457.  689.  278.   97.  114.  744.  196.  133.   53.\n",
      "  741.  571.  134.  297.   93.  131.  413.  456.   48.  273.   99.  756.\n",
      "  102.  658.  857.  518.  664.  686.   45.]\n",
      "79 79\n"
     ]
    }
   ],
   "source": [
    "# with open('../Data/baseline_left_labels.pkl', 'rb') as f:\n",
    "#     corr = pickle.load(f)\n",
    "\n",
    "# print(corr['nums'])\n",
    "# print(len(corr['adj']), len(corr['nums']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59601, 3) (14815, 3)\n",
      "Index(['image_id', 'object_names', 'object_ids'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "vg_ml_train = pd.read_csv(\"../Data/visual_genome/visual_genome_train_ann_ml.csv\")\n",
    "vg_ml_val = pd.read_csv(\"../Data/visual_genome/visual_genome_val_ann_ml.csv\")\n",
    "print(vg_ml_train.shape, vg__ml_val.shape)\n",
    "print(vg_ml_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_labels(df):\n",
    "    sample_list = []  # append one row at a time to this\n",
    "    \n",
    "    nrows = df.shape[0]\n",
    "    count = 0\n",
    "    for r in range(nrows):  # generate n rows for each row in original table\n",
    "        list_label = literal_eval(df['object_ids'].iloc[r])\n",
    "        list_objects = literal_eval(df['object_names'].iloc[r])\n",
    "        img_id = df['image_id'].iloc[r]\n",
    "        \n",
    "        n = len(list_label)\n",
    "        count += n\n",
    "        for i in range(n):\n",
    "            new_row = []\n",
    "            new_row.append(img_id)\n",
    "            new_row.append([list_objects[i]]) # add as a list of size = 1\n",
    "            new_row.append([list_label[i]])\n",
    "            sample_list.append(new_row)\n",
    "    \n",
    "    print('count', count)\n",
    "    print(len(sample_list), len(sample_list[0]))\n",
    "    final_columns = ['image_id', 'object_names', 'object_ids']\n",
    "    final_df = pd.DataFrame(sample_list,columns=final_columns)\n",
    "    print(final_df.shape)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 111622\n",
      "111622 3\n",
      "(111622, 3)\n",
      "59601\n",
      "count 27929\n",
      "27929 3\n",
      "(27929, 3)\n",
      "14815\n"
     ]
    }
   ],
   "source": [
    "train_df = split_labels(vg_ml_train)\n",
    "print(train_df.image_id.nunique())\n",
    "train_df.to_csv('../Data/visual_genome/visual_genome_train_ann.csv', index = False)\n",
    "\n",
    "val_df = split_labels(vg_ml_val)\n",
    "print(val_df.image_id.nunique())\n",
    "val_df.to_csv('../Data/visual_genome/visual_genome_val_ann.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_ml_df = pd.read_csv('../data/visual_genome/visual_genome_ann.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 174424\n",
      "174424 3\n",
      "(174424, 3)\n",
      "93016\n"
     ]
    }
   ],
   "source": [
    "vg_df = split_labels(vg_ml_df)\n",
    "print(vg_df.image_id.nunique())\n",
    "vg_df.to_csv('../data/visual_genome/visual_genome_ann.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>object_names</th>\n",
       "      <th>object_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[parking meter]</td>\n",
       "      <td>[12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[clock]</td>\n",
       "      <td>[74]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[car]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>[backpack]</td>\n",
       "      <td>[24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[car]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id     object_names object_ids\n",
       "0         1  [parking meter]       [12]\n",
       "1         1          [clock]       [74]\n",
       "2         1            [car]        [2]\n",
       "3         2       [backpack]       [24]\n",
       "4         2            [car]        [2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_temp = pd.read_csv('../data/visual_genome/visual_genome_gcn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 174424\n",
      "174424 3\n",
      "(174424, 3)\n"
     ]
    }
   ],
   "source": [
    "vg_temp = split_labels(vg_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b239d48829f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvg_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m79\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mvg_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m77\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mvg_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mvg_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "for index, row in vg_temp.iterrows():\n",
    "    if index % 1000 == 0:\n",
    "        print(index)\n",
    "    \n",
    "    if vg_temp.iloc[index, 2] == 79:\n",
    "        vg_temp.iloc[index, 2] = 77\n",
    "    elif vg_temp.iloc[index, 2] > 32:\n",
    "        vg_temp.iloc[index, 2] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
